{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "from hashlib import md5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Defining your job inputs\n",
    "\n",
    "The following cell is tagged with \"parameters\", which allows papermill to identify the cell containing per-run parameters\n",
    "Cell tags may be accessed using the double-gear icon in JupyterLab's left-hand gutter.\n",
    "\n",
    "All variables defined in the following cell are treated as job input parameters, and will be accessible through the `_context.json` file at runtime.\n",
    "\n",
    "For more information, visit https://papermill.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Job input parameters\n",
    "\n",
    "polygon: Dict = {\n",
    "  \"type\": \"Feature\",\n",
    "  \"geometry\": {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [[-180, -90], [-180, 90], [180, 90], [180, -90], [-180, -90]]\n",
    "  },\n",
    "  \"properties\": {\n",
    "    \"name\": \"Earth\"\n",
    "  }\n",
    "}\n",
    "track_number: int = 0\n",
    "    \n",
    "active_pixel_x_min: int = 20\n",
    "active_pixel_x_max: int = 30\n",
    "active_pixel_y_min: int = 20\n",
    "active_pixel_y_max: int = 30    \n",
    "\n",
    "outer_pixel_x_min: int = 15\n",
    "outer_pixel_x_max: int = 35\n",
    "outer_pixel_y_min: int = 15\n",
    "outer_pixel_y_max: int = 35\n",
    "\n",
    "start_date: str = \"2000-01-01\"\n",
    "end_date: str = \"2100-01-01\"\n",
    "\n",
    "# PCM-System Parameters\n",
    "# These use reserved-prefix parameter names (_*) and are also parsed during `notebook-pge-wrapper specs` to generate the hysds-io and job-spec\n",
    "_time_limit = 600\n",
    "_soft_time_limit = 600\n",
    "_disk_usage = \"1GB\"\n",
    "_submission_type = \"individual\"\n",
    "_required_queue = \"factotum-job_worker-small\"\n",
    "_label = \"Create Volcano Anomaly AOI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Defining your job outputs and metadata files\n",
    "\n",
    "The following cell contains the functions necessary to create a trivial data product for ingestion into the PCM data product catalog.\n",
    "\n",
    "These functions should be augmented to include your desired dataset definition data, metadata and job output files\n",
    "\n",
    "It is also typical to include important fields (e.g. track number, orbit direction and temporal bound timestamps) in the dataset id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = os.path.abspath(os.curdir)\n",
    "\n",
    "def generate_dummy_context_file() -> None:\n",
    "    \"\"\"When run in HySDS, a _context.json file will be present in the working directory, so this is only necessary for local development\"\"\"\n",
    "    filepath: str = os.path.join(working_dir, '_context.json')\n",
    "    print(f'Writing dummy context to {filepath}')\n",
    "    with open(filepath, 'w+') as context_file:\n",
    "        json.dump({'run_timestamp': datetime.now().isoformat()}, context_file)\n",
    "\n",
    "def generate_dataset_id(id_prefix: str, context_str: str) -> str:\n",
    "    \"\"\"Generates a globally-unique ID for the data product produced.\n",
    "    Uniqueness is generally ensured by the context, which will (theoretically) be either unique, or subject to deduplication by HySDS\"\"\"\n",
    "    \n",
    "    hash_suffix = md5(context_str.encode()).hexdigest()[0:5]\n",
    "\n",
    "    job_id = f'{id_prefix}-{datetime.now().isoformat()}-{hash_suffix}'\n",
    "\n",
    "    print(f'Generated job ID: {job_id}')\n",
    "    return job_id\n",
    "\n",
    "\n",
    "def generate_dataset_file(dataset_id: str, **kwargs) -> None:\n",
    "    \"\"\"Stores standardized metadata used for indexing products in HySDS GRQ\"\"\"\n",
    "    dataset_definition_filepath: str = os.path.join(working_dir, dataset_id, f'{dataset_id}.dataset.json')\n",
    "    metadata: dict = {\n",
    "        'version': kwargs.get('version', 'v1.0'),\n",
    "    }\n",
    "    \n",
    "    optional_fields = [\n",
    "        'label',\n",
    "        'location',  # Must adhere to geoJSON \"geometry\" format\n",
    "        'starttime',\n",
    "        'endtime'\n",
    "    ]\n",
    "    for field in optional_fields:\n",
    "        if field in kwargs:\n",
    "            metadata[field] = kwargs.get(field)\n",
    "    \n",
    "    with open(dataset_definition_filepath, 'w+') as dataset_file:\n",
    "        print(f'Writing to {dataset_definition_filepath}')\n",
    "        json.dump(metadata, dataset_file)\n",
    "    \n",
    "def generate_metadata_file(dataset_id: str, metadata: Dict) -> None:\n",
    "    \"\"\"Stores custom metadata keys/values used for indexing products in HySDS GRQ\"\"\"\n",
    "    metadata_filepath: str = os.path.join(working_dir, dataset_id, f'{dataset_id}.met.json')\n",
    "\n",
    "    with open(metadata_filepath, 'w+') as metadata_file:\n",
    "        print(f'Writing to {metadata_filepath}')\n",
    "        json.dump(metadata, metadata_file)\n",
    "        \n",
    "\n",
    "        \n",
    "def generate_data_product(working_dir: str = working_dir, id_prefix: str = 'VOLCANO_ANOMALY_AOI') -> None:\n",
    "    \"\"\"Generates metadata/dataset files and packages them in a specially-named directory with the desired job output files, for ingestion into the data product catalog\"\"\"\n",
    "    context_filepath: str = os.path.join(working_dir, '_context.json') \n",
    "    with open(context_filepath) as context_file:\n",
    "        context_str: str = context_file.read()\n",
    "            \n",
    "    context = json.loads(context_str)\n",
    "            \n",
    "    dataset_id: str = generate_dataset_id(id_prefix, context_str)\n",
    "        \n",
    "    params = {}\n",
    "    for param in context['job_specification']['params']:\n",
    "        params[param['name']] = param['value']\n",
    "    \n",
    "    data_product_dir = os.path.join(working_dir, dataset_id)\n",
    "    print(f'Generating data product at {data_product_dir}')\n",
    "    \n",
    "    os.mkdir(data_product_dir)\n",
    "    \n",
    "    data_product = params\n",
    "    \n",
    "    print(data_product)\n",
    "    \n",
    "    generate_metadata_file(dataset_id, data_product)\n",
    "    \n",
    "    geo_json = data_product.get('polygon')\n",
    "    \n",
    "    label = geo_json.get('properties').get('name')\n",
    "    location = {\n",
    "      \"coordinates\": [\n",
    "        [\n",
    "          [\n",
    "            [\n",
    "              -96.419273,\n",
    "              21.798433\n",
    "            ],\n",
    "            [\n",
    "              -96.074295,\n",
    "              23.425812\n",
    "            ],\n",
    "            [\n",
    "              -98.505028,\n",
    "              23.842773\n",
    "            ],\n",
    "            [\n",
    "              -98.819763,\n",
    "              22.218616\n",
    "            ],\n",
    "            [\n",
    "              -96.419273,\n",
    "              21.798433\n",
    "            ]\n",
    "          ]\n",
    "        ]\n",
    "      ],\n",
    "      \"type\": \"MultiPolygon\"\n",
    "    }\n",
    "    starttime = datetime.strptime(data_product.get('start_date'), '%Y-%m-%d').isoformat()\n",
    "    endtime = datetime.strptime(data_product.get('end_date'), '%Y-%m-%d').isoformat()\n",
    "    \n",
    "    generate_dataset_file(dataset_id,\n",
    "                          label=label,\n",
    "                          location=location,\n",
    "                          starttime=starttime,\n",
    "                          endtime=endtime\n",
    "                         )\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Defining your job's high-level execution flow\n",
    "\n",
    "The following cell contains a trivial set of procedural calls, which will be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated job ID: VOLCANO_ANOMALY_AOI-2021-06-15T23:22:30.399944-a4909\n",
      "Generating data product at /home/jovyan/create_volcano_anomaly_aoi/notebook_pges/VOLCANO_ANOMALY_AOI-2021-06-15T23:22:30.399944-a4909\n",
      "{'polygon': {'geometry': {'coordinates': [[-180, -90], [-180, 90], [180, 90], [180, -90], [-180, -90]], 'type': 'Polygon'}, 'properties': {'name': 'Earth'}, 'type': 'Feature'}, 'track_number': 124.0, 'active_pixel_x_min': 20.0, 'active_pixel_x_max': 30.0, 'active_pixel_y_min': 20.0, 'active_pixel_y_max': 30.0, 'outer_pixel_x_min': 15.0, 'outer_pixel_x_max': 35.0, 'outer_pixel_y_min': 15.0, 'outer_pixel_y_max': 35.0, 'start_date': '2000-01-01', 'end_date': '2100-01-01'}\n",
      "Writing to /home/jovyan/create_volcano_anomaly_aoi/notebook_pges/VOLCANO_ANOMALY_AOI-2021-06-15T23:22:30.399944-a4909/VOLCANO_ANOMALY_AOI-2021-06-15T23:22:30.399944-a4909.met.json\n",
      "Writing to /home/jovyan/create_volcano_anomaly_aoi/notebook_pges/VOLCANO_ANOMALY_AOI-2021-06-15T23:22:30.399944-a4909/VOLCANO_ANOMALY_AOI-2021-06-15T23:22:30.399944-a4909.dataset.json\n",
      "PGE execution complete!\n"
     ]
    }
   ],
   "source": [
    "generate_data_product()\n",
    "\n",
    "print('PGE execution complete!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
